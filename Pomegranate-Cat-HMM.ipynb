{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "import numpy as np\n",
    "import pandas as pd # used to display data more cleanly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we will look at a Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep                0.60\n",
      "eat                  0.05\n",
      "destroy_furniture    0.10\n",
      "make_videos          0.25\n",
      "Name: states, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "states = ['sleep', # cats sleep for 60% of their lives\n",
    "          'eat',   # cats are crepuscular, active during twilight\n",
    "          'destroy_furniture',  # cats contribute to 10.2% of annual revenues in the furniture industry\n",
    "          'make_videos'] # more than 2 million cat videos on YouTube\n",
    "pi = [0.6, 0.05, 0.1, 0.25]\n",
    "\n",
    "# create state space and initial state probabilities\n",
    "\n",
    "state_space = pd.Series(pi, index=states, name='states')\n",
    "print(state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  sleep   eat destroy_furniture make_videos\n",
      "sleep               0.4   0.2              0.05        0.35\n",
      "eat                 0.8  0.05               0.1        0.05\n",
      "destroy_furniture   0.6  0.05               0.1        0.25\n",
      "make_videos         0.1   0.4               0.1         0.4\n",
      "\n",
      " [[0.4 0.2 0.05 0.35]\n",
      " [0.8 0.05 0.1 0.05]\n",
      " [0.6 0.05 0.1 0.25]\n",
      " [0.1 0.4 0.1 0.4]] (4, 4) \n",
      "\n",
      "sleep                1.0\n",
      "eat                  1.0\n",
      "destroy_furniture    1.0\n",
      "make_videos          1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The next step is to define the transition probabilities. \n",
    "# They are simply the probabilities of staying in the same state \n",
    "# or moving to a different state given the current state. \n",
    "# create transition matrix\n",
    "# equals transition probability matrix of changing states given a state\n",
    "# matrix is size (M x M) where M is number of states\n",
    "\n",
    "q_df = pd.DataFrame(columns=states, index=states)\n",
    "#      {state} -> [sleeping, eating, dest_furn, make_vids]\n",
    "q_df.loc[states[0]] = [0.4, 0.2, 0.05, 0.35] # sleep \n",
    "q_df.loc[states[1]] = [0.8, 0.05, 0.1, 0.05] # eating\n",
    "q_df.loc[states[2]] = [0.6, 0.05, 0.1, 0.25] # dest_furn\n",
    "q_df.loc[states[3]] = [0.1, 0.4, 0.1, 0.4] #make_vids\n",
    "\n",
    "print(q_df)\n",
    "\n",
    "q = q_df.values\n",
    "print('\\n', q, q.shape, '\\n')\n",
    "print(q_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = DiscreteDistribution({'sleep': 0.6, \n",
    "                           'eat': 0.05,\n",
    "                           'destroy_furniture': 0.1, \n",
    "                           'make_videos': 0.25})\n",
    "\n",
    "d2 = ConditionalProbabilityTable([['sleep', 'sleep', 0.4],\n",
    "                                  ['sleep', 'eat', 0.2],\n",
    "                                  ['sleep', 'destroy_furniture', 0.05],\n",
    "                                  ['sleep', 'make_videos', 0.35],\n",
    "                                  ['eat', 'sleep', 0.8],\n",
    "                                  ['eat', 'eat', 0.05],\n",
    "                                  ['eat', 'destroy_furniture', 0.1],\n",
    "                                  ['eat', 'make_videos', 0.05], \n",
    "                                  ['destroy_furniture', 'sleep', 0.6],\n",
    "                                  ['destroy_furniture', 'eat', 0.05],\n",
    "                                  ['destroy_furniture', 'destroy_furniture', 0.1],\n",
    "                                  ['destroy_furniture', 'make_videos', 0.25],\n",
    "                                  ['destroy_furniture', 'sleep', 0.1],\n",
    "                                  ['destroy_furniture', 'eat', 0.4],\n",
    "                                  ['destroy_furniture', 'destroy_furniture', 0.1], \n",
    "                                  ['destroy_furniture', 'make_videos', 0.4]],\n",
    "                                [d1])\n",
    "\n",
    "clf = MarkovChain([d1, d2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov chains have log probability, fit, summarize, and from summaries methods implemented. They do not have classification capabilities by themselves, but when combined with a Naive Bayes classifier can be used to do discrimination between multiple models (see the Naive Bayes tutorial notebook).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.03865410927848"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.log_probability( [\"eat\", \"sleep\", \"eat\", \"eat\", \"eat\", \"destroy_furniture\", \"make_videos\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5108256237659907"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.log_probability( [\"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3434070875143007"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.log_probability( [\"sleep\" , \"sleep\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can fit the model to sequences which we pass in, and as expected, get better performance on sequences which we train on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.3561076606958915\n",
      "-0.40546510810816444\n",
      "-1.2163953243244934\n"
     ]
    }
   ],
   "source": [
    "clf.fit([[\"eat\", \"sleep\", \"eat\", \"eat\", \"eat\", \"destroy_furniture\", \"make_videos\"],\n",
    "         [\"sleep\"],\n",
    "         [\"sleep\" , \"sleep\", \"sleep\"]])\n",
    "\n",
    "print(clf.log_probability([\"eat\", \"sleep\", \"eat\", \"eat\", \"eat\", \"destroy_furniture\", \"make_videos\"]))\n",
    "print(clf.log_probability([\"sleep\"]))\n",
    "print(clf.log_probability([\"sleep\" , \"sleep\", \"sleep\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"class\" :\"Distribution\",\n",
      "    \"name\" :\"DiscreteDistribution\",\n",
      "    \"parameters\" :[\n",
      "        {\n",
      "            \"sleep\" :0.6666666666666666,\n",
      "            \"eat\" :0.3333333333333333,\n",
      "            \"destroy_furniture\" :0.0,\n",
      "            \"make_videos\" :0.0\n",
      "        }\n",
      "    ],\n",
      "    \"frozen\" :false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(clf.distributions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\tsleep\t0.6666666666666666\n",
      "sleep\teat\t0.3333333333333333\n",
      "sleep\tdestroy_furniture\t0.0\n",
      "sleep\tmake_videos\t0.0\n",
      "eat\tsleep\t0.25\n",
      "eat\teat\t0.5\n",
      "eat\tdestroy_furniture\t0.25\n",
      "eat\tmake_videos\t0.0\n",
      "destroy_furniture\tsleep\t0.0\n",
      "destroy_furniture\teat\t0.0\n",
      "destroy_furniture\tdestroy_furniture\t0.0\n",
      "destroy_furniture\tmake_videos\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(clf.distributions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let us look at a Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increasing = DiscreteDistribution({'sleep': 0.5, \n",
    "                                   'eat': 0.2,\n",
    "                                   'dest': 0.05,                            \n",
    "                                   'vids': 0.25})\n",
    "\n",
    "decreasing = DiscreteDistribution({'sleep': 0.2, \n",
    "                                   'eat': 0.05,\n",
    "                                   'dest': 0.4,                            \n",
    "                                   'vids': 0.35})\n",
    "\n",
    "s1 = State( increasing, name=\"increasing\" )\n",
    "s2 = State( decreasing, name=\"decreasing\" )\n",
    "\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.add_states(s1, s2)\n",
    "\n",
    "hmm.add_transition(hmm.start, s1, 0.8)\n",
    "hmm.add_transition(hmm.start, s2, 0.2)\n",
    "hmm.add_transition(s1, s1, 0.8)\n",
    "hmm.add_transition(s1, s2, 0.2)\n",
    "hmm.add_transition(s2, s1, 0.2)\n",
    "hmm.add_transition(s2, s2, 0.8)\n",
    "hmm.bake()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: decreasing\n",
      "1: increasing\n",
      "\n",
      "sequence: sleep\teat\tsleep\tvids\tsleep\tdest\tsleep\teat\tvids\tvids\n",
      "hmm pred: 1\t1\t1\t1\t1\t0\t1\t1\t1\t0\n",
      "\n",
      "sequence: dest\tvids\tdest\tvids\tdest\tvids\tsleep\tsleep\tsleep\n",
      "hmm pred: 0\t0\t0\t0\t0\t0\t1\t1\t1\n"
     ]
    }
   ],
   "source": [
    "seq1 = ['sleep', 'eat', 'sleep', 'vids', 'sleep', 'dest', 'sleep', 'eat', 'vids', 'vids']\n",
    "seq2 = ['dest', 'vids', 'dest', 'vids', 'dest', 'vids', 'sleep', 'sleep', 'sleep']\n",
    "\n",
    "print(\"0:\", hmm.states[0].name)\n",
    "print(\"1:\", hmm.states[1].name, end=\"\\n\\n\")\n",
    "\n",
    "predictions1 = hmm.predict( seq1 )\n",
    "print(\"sequence: {}\".format( \"\\t\".join( seq1 )))\n",
    "print(\"hmm pred: {}\".format( \"\\t\".join( map( str, predictions1)) ), end=\"\\n\\n\")\n",
    "\n",
    "predictions2 = hmm.predict( seq2 )\n",
    "print(\"sequence: {}\".format( \"\\t\".join( seq2 )))\n",
    "print(\"hmm pred: {}\".format( \"\\t\".join( map( str, predictions2)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: decreasing\n",
      "state 1: increasing\n",
      "\n",
      "        decreasing  increasing \t\tprediction\n",
      "sleep\t[0.03775431 0.96224569]\t\t1\n",
      "eat\t[0.05113983 0.94886017]\t\t1\n",
      "sleep\t[0.12944284 0.87055716]\t\t1\n",
      "vids\t[0.31235225 0.68764775]\t\t1\n",
      "sleep\t[0.35368476 0.64631524]\t\t1\n",
      "dest\t[0.63454255 0.36545745]\t\t0\n",
      "sleep\t[0.31494002 0.68505998]\t\t1\n",
      "eat\t[0.23641257 0.76358743]\t\t1\n",
      "vids\t[0.43505411 0.56494589]\t\t1\n",
      "vids\t[0.51560427 0.48439573]\t\t0\n"
     ]
    }
   ],
   "source": [
    "probs1 = hmm.predict_proba( seq1 )\n",
    "\n",
    "print(\"state 0:\", hmm.states[0].name)\n",
    "print(\"state 1:\", hmm.states[1].name)\n",
    "print()\n",
    "\n",
    "print (\"       \", hmm.states[0].name, \"\", hmm.states[1].name,\"\\t\\tprediction\")\n",
    "for i in range(len(seq1)):\n",
    "    print(seq1[i], probs1[i], \"\", predictions1[i],sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: decreasing\n",
      "state 1: increasing\n",
      "\n",
      "        decreasing  increasing \t\tprediction\n",
      "dest\t[0.03775431 0.96224569]\t\t0\n",
      "vids\t[0.05113983 0.94886017]\t\t0\n",
      "dest\t[0.12944284 0.87055716]\t\t0\n",
      "vids\t[0.31235225 0.68764775]\t\t0\n",
      "dest\t[0.35368476 0.64631524]\t\t0\n",
      "vids\t[0.63454255 0.36545745]\t\t0\n",
      "sleep\t[0.31494002 0.68505998]\t\t1\n",
      "sleep\t[0.23641257 0.76358743]\t\t1\n",
      "sleep\t[0.43505411 0.56494589]\t\t1\n"
     ]
    }
   ],
   "source": [
    "probs2 = hmm.predict_proba( seq1 )\n",
    "\n",
    "print(\"state 0:\", hmm.states[0].name)\n",
    "print(\"state 1:\", hmm.states[1].name)\n",
    "print()\n",
    "\n",
    "print (\"       \", hmm.states[0].name, \"\", hmm.states[1].name,\"\\t\\tprediction\")\n",
    "for i in range(len(seq2)):\n",
    "    print(seq2[i], probs2[i], \"\", predictions2[i],sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the emission probability values calculated by the forward backward algorithm, and can also be retrieved by calling the `hmm.forward_backward( seq )`, which returns the emissions and the transition probability tables.\n",
    "\n",
    "The emissions are the log probabilities, which we have already seen converted into normal probabilities above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.77764161 0.72768163 0.         0.        ]\n",
      " [1.20553159 5.28914517 0.         0.        ]\n",
      " [0.03775431 0.96224569 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "trans, ems = hmm.forward_backward(seq1)\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the transition table, which has the soft count of the number of transitions across an edge in the model given a single sequence. It is a square matrix of size equal to the number of states (including start and end state), with number of transitions from (row_id) to (column_id). This is exemplified by the 1.0 in the first row, indicating that there is one transition from background state to the end state, as that's the only way to reach the end state. However, the third (or fourth, depending on ordering) row is the transitions from the start state, and it only slightly favors the background state. These counts are not normalized to the length of the input sequence, but can easily be done so by dividing by row sums, column sums, or entire table sums, depending on your application.\n",
    "\n",
    "A possible reason not to normalize is to run several sequences through and add up their tables, because normalizing in the end and extracting some domain knowledge. It is extremely useful in practice. For example, we can see that there is an expectation of 2.8956 transitions from CG island to background, and 2.4 from background to CG island. This could be used to infer that there are ~2-3 edges, which makes sense if you consider that the start and end of the sequence seem like they might be part of the CG island states except for the strict transition probabilities used (look at the first few rows of the emission table above.)\n",
    "\n",
    "We've been using the forward backward algorithm and maximum a posteriori for decoding thus far, however maximum a posteriori decoding has the side effect that it is possible that it predicts impossible sequences in some edge cases. An alternative is Viterbi decoding, which at each step takes the most likely path, instead of sum of all paths, to produce hard assignments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm state 0: decreasing\n",
      "hmm state 1: increasing\n",
      "\n",
      "sequence1: sleep\teat\tsleep\tvids\tsleep\tdest\tsleep\teat\tvids\tvids\n",
      "hmm pred1: 1\t1\t1\t1\t1\t1\t1\t1\t1\t1\n",
      "\n",
      "sequence2: dest\tvids\tdest\tvids\tdest\tvids\tsleep\tsleep\tsleep\n",
      "hmm pred: 0\t0\t0\t0\t0\t0\t1\t1\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"hmm state 0: {}\".format( hmm.states[0].name ))\n",
    "print(\"hmm state 1: {}\".format( hmm.states[1].name ))\n",
    "print()\n",
    "\n",
    "hmm_predictions1 = hmm.predict( seq1, algorithm='viterbi' )[1:]\n",
    "print(\"sequence1: {}\".format( '\\t'.join( seq1 ) ))\n",
    "print(\"hmm pred1: {}\".format( '\\t'.join( map( str, hmm_predictions1 ) ) ))\n",
    "\n",
    "print()\n",
    "hmm_predictions2 = hmm.predict( seq2, algorithm='viterbi' )[1:]\n",
    "print(\"sequence2: {}\".format( '\\t'.join( seq2 ) ))\n",
    "print(\"hmm pred: {}\".format( '\\t'.join( map( str, hmm_predictions2 ) ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training HMM's\n",
    "There are two main algorithms for training hidden Markov models-- Baum Welch (structured version of Expectation Maximization), and Viterbi training. **Since we don't start off with labels on the data, these are both unsupervised training algorithms.** In order to assign labels, Baum Welch uses EM to assign soft labels (weights in this case) to each point belonging to each state, and then using weighted MLE estimates to update the distributions. Viterbi assigns hard labels to each observation using the Viterbi algorithm, and then updates the distributions based on these hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increasing = DiscreteDistribution({'sleep': 0.5, \n",
    "                                   'eat': 0.2,\n",
    "                                   'dest': 0.05,                            \n",
    "                                   'vids': 0.25})\n",
    "\n",
    "decreasing = DiscreteDistribution({'sleep': 0.2, \n",
    "                                   'eat': 0.05,\n",
    "                                   'dest': 0.4,                            \n",
    "                                   'vids': 0.35})\n",
    "\n",
    "s1 = State( increasing, name=\"increasing\" )\n",
    "s2 = State( decreasing, name=\"decreasing\" )\n",
    "\n",
    "hmm2 = HiddenMarkovModel()\n",
    "hmm2.add_states(s1, s2)\n",
    "\n",
    "hmm2.add_transition(hmm2.start, s1, 0.8)\n",
    "hmm2.add_transition(hmm2.start, s2, 0.2)\n",
    "hmm2.add_transition(s1, s1, 0.8)\n",
    "hmm2.add_transition(s1, s2, 0.2)\n",
    "hmm2.add_transition(s2, s1, 0.2)\n",
    "hmm2.add_transition(s2, s2, 0.8)\n",
    "hmm2.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0516327737924627"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm2.fit( [seq1, seq2,\n",
    "           ['sleep'], \n",
    "          ['eat', 'dest', 'vids'],\n",
    "          ['sleep', 'sleep', 'sleep', 'sleep'],\n",
    "          ['dest', 'sleep', 'dest', 'sleep', 'dest', 'eat']\n",
    "         ],\n",
    "         algorithm='baum-welch',\n",
    "         max_iterations=5,\n",
    "         distribution_inertia=0.3,\n",
    "         edge_inertia=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.267430183699856"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm2.fit( [['eat', 'dest', 'sleep'], \n",
    "          ['dest', 'eat', 'sleep'],\n",
    "          ['eat', 'eat', 'eat', 'eat'],\n",
    "          ['dest', 'eat', 'sleep', 'sleep', 'sleep', 'eat']\n",
    "         ],\n",
    "         algorithm='baum-welch',\n",
    "         max_iterations=5,\n",
    "         distribution_inertia=0.3,\n",
    "         edge_inertia=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting On the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: dest\teat\tdest\teat\tvids\tvids\tsleep\n",
      "hmm pred: 0\t1\t0\t1\t1\t1\t1\n",
      "\n",
      "sequence: dest\tvids\tdest\tvids\tdest\tvids\tdest\tsleep\tsleep\tsleep\n",
      "hmm pred: 0\t0\t0\t0\t0\t0\t0\t1\t1\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq3 = ['dest', 'eat', 'dest', 'eat', 'vids', 'vids', 'sleep']\n",
    "seq4 = ['dest', 'vids', 'dest', 'vids', 'dest', 'vids', 'dest', 'sleep', 'sleep', 'sleep']\n",
    "\n",
    "predictions_hmm2 = hmm.predict( seq3 )\n",
    "print(\"sequence: {}\".format( \"\\t\".join( seq3 )))\n",
    "print(\"hmm pred: {}\".format( \"\\t\".join( map( str, predictions_hmm2)) ), end=\"\\n\\n\")\n",
    "\n",
    "predictions_hmm2 = hmm.predict( seq4 )\n",
    "print(\"sequence: {}\".format( \"\\t\".join( seq4 )))\n",
    "print(\"hmm pred: {}\".format( \"\\t\".join( map( str, predictions_hmm2)) ), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has our model learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.14863232 0.88474332 0.         0.        ]\n",
      " [0.63318129 2.33344306 0.         0.        ]\n",
      " [0.57970635 0.42029365 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "[[1.94917592 1.1752278  0.         0.        ]\n",
      " [0.19121057 2.68438572 0.         0.        ]\n",
      " [0.98840894 0.01159106 0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "trans1, _ = hmm.forward_backward(seq3)\n",
    "print(trans1)\n",
    "\n",
    "trans2, _ = hmm2.forward_backward(seq3)\n",
    "print(trans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
